import sys
import os
import glob
import torch
import numpy as np
import itertools 
import time
import warnings 

# è¿‡æ»¤ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="monai.inferers.utils")

# --- è·¯å¾„é…ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.inferers import sliding_window_inference
from monai.utils import set_determinism
from monai.data import decollate_batch, partition_dataset
from monai.transforms import AsDiscrete

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.models.unet3D import UNet3D
from src.dataloaders.basic_loader import get_basic_loader
from src.ssl.utils import update_ema_variables, get_current_consistency_weight, ConsistencyLoss
from src.ssl.tmo import TMOAdamW  # [æ ¸å¿ƒ] å¯¼å…¥ TMO ä¼˜åŒ–å™¨

def train_tmo():
    # ================= é…ç½®åŒºåŸŸ =================
    DATA_DIR = "/home/lzf/Code/dataset/nnUNet_raw/Dataset701_STS3D_ROI"
    MODEL_SAVE_DIR = "./weights/ssl_tmo"
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
    
    # è®­ç»ƒè¶…å‚æ•°
    MAX_EPOCHS = 5
    VAL_INTERVAL = 2
    LR = 1e-4
    ROI_SIZE = (64, 64, 64)
    
    # æ¯”ä¾‹é…ç½® (Labeled : Unlabeled)
    # TMO ä¸¥é‡ä¾èµ–æœ‰æ ‡ç­¾æ¢¯åº¦çš„è´¨é‡ï¼Œå»ºè®® BATCH_SIZE_L ä¸è¦å¤ªå°
    BATCH_SIZE_L = 1
    BATCH_SIZE_U = 1
    
    # Mean Teacher & Consistency å‚æ•°
    EMA_DECAY = 0.99       
    CONSISTENCY = 0.1      
    CONSISTENCY_RAMPUP = 20 
    
    # èµ„æºé…ç½®
    NUM_WORKERS = 0
    CACHE_RATE = 0.0

    # ================= 1. æ•°æ®å‡†å¤‡ =================
    set_determinism(seed=2025)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ å¼€å§‹ TMO (Trusted Momentum) è®­ç»ƒ | è®¾å¤‡: {device}")

    # A. æœ‰æ ‡ç­¾æ•°æ®
    labeled_images = sorted(glob.glob(os.path.join(DATA_DIR, "imagesTr", "*.nii.gz")))
    labeled_labels = sorted(glob.glob(os.path.join(DATA_DIR, "labelsTr", "*.nii.gz")))
    labeled_dicts = [{"image": i, "label": l} for i, l in zip(labeled_images, labeled_labels)]
    
    # åˆ’åˆ† Train/Val
    train_labeled_files, val_files = partition_dataset(
        data=labeled_dicts, ratios=[0.8, 0.2], shuffle=True, seed=2025
    )

    # B. æ— æ ‡ç­¾æ•°æ®
    unlabeled_dir = os.path.join(DATA_DIR, "imagesUnlabeled")
    if os.path.exists(unlabeled_dir):
        unlabeled_images = sorted(glob.glob(os.path.join(unlabeled_dir, "*.nii.gz")))
        unlabeled_dicts = [{"image": i, "label": i} for i in unlabeled_images]
        print(f"  - æœ‰æ ‡ç­¾æ•°æ®: {len(train_labeled_files)}")
        print(f"  - æ— æ ‡ç­¾æ•°æ®: {len(unlabeled_dicts)}")
    else:
        print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° imagesUnlabeledï¼ŒTMO å°†å›é€€åˆ°ä¼ªSSLæ¨¡å¼ï¼")
        unlabeled_dicts = train_labeled_files 

    # C. åˆ›å»ºåŠ è½½å™¨
    loader_labeled = get_basic_loader(
        data_list=train_labeled_files,
        batch_size=BATCH_SIZE_L, 
        roi_size=ROI_SIZE, 
        is_train=True, 
        num_workers=NUM_WORKERS,
        cache_rate=CACHE_RATE,
        limit=1
    )
    
    loader_unlabeled = get_basic_loader(
        data_list=unlabeled_dicts,
        batch_size=BATCH_SIZE_U, 
        roi_size=ROI_SIZE, 
        is_train=True, 
        num_workers=NUM_WORKERS,
        cache_rate=CACHE_RATE,
        limit=1
    )
    
    loader_val = get_basic_loader(
        data_list=val_files,
        batch_size=1, 
        roi_size=ROI_SIZE, 
        is_train=False, 
        num_workers=NUM_WORKERS,
        cache_rate=CACHE_RATE,
        limit=1
    )

    # ================= 2. æ¨¡å‹ä¸ä¼˜åŒ–å™¨ =================
    def create_model():
        return UNet3D(in_channels=1, out_channels=2).to(device)

    model = create_model()      # Student
    ema_model = create_model()  # Teacher (EMA)

    # åˆå§‹åŒ– Teacher
    for param in ema_model.parameters():
        param.detach_()
    ema_model.load_state_dict(model.state_dict())

    # æŸå¤±å‡½æ•°
    loss_supervised = DiceCELoss(to_onehot_y=True, softmax=True)
    loss_consistency = ConsistencyLoss()
    
    # [æ ¸å¿ƒ] ä½¿ç”¨ TMO ä¼˜åŒ–å™¨
    optimizer = TMOAdamW(model.parameters(), lr=LR)
    
    dice_metric = DiceMetric(include_background=False, reduction="mean")

    # ================= 3. è®­ç»ƒå¾ªç¯ =================
    best_metric = -1
    
    print(f"\n{'='*20} Start TMO Training {'='*20}")

    for epoch in range(MAX_EPOCHS):
        epoch_start = time.time()
        model.train()
        ema_model.train()
        
        loss_sup_sum = 0
        loss_cons_sum = 0
        step = 0
        
        consistency_weight = get_current_consistency_weight(epoch, MAX_EPOCHS, CONSISTENCY, CONSISTENCY_RAMPUP)
        
        # å¾ªç¯è¿­ä»£
        train_iterator = zip(loader_unlabeled, itertools.cycle(loader_labeled))
        
        for batch_u, batch_l in train_iterator:
            step += 1
            
            # æ•°æ®å‡†å¤‡
            img_l, lbl_l = batch_l["image"].to(device), batch_l["label"].to(device)
            img_u = batch_u["image"].to(device)
            
            # -------------------------------------------------------
            # [æ­¥éª¤ 1] Labeled Step: å»ºç«‹å¯ä¿¡æ–¹å‘ (Trusted Direction)
            # -------------------------------------------------------
            optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
            
            pred_l = model(img_l)
            loss_sup = loss_supervised(pred_l, lbl_l)
            
            # åå‘ä¼ æ’­äº§ç”Ÿ g_L
            loss_sup.backward()
            optimizer.step_labeled()
            
            # -------------------------------------------------------
            # [æ­¥éª¤ 2] Unlabeled Step: è°¨æ…æ›´æ–° (Cautious Update)
            # -------------------------------------------------------
            optimizer.zero_grad() # æ¸…ç©º g_Lï¼Œå‡†å¤‡è®¡ç®— g_U
            
            # Student Forward
            pred_u_student = model(img_u)
            # Teacher Forward (No Grad)
            with torch.no_grad():
                pred_u_teacher = ema_model(img_u)
            
            # è®¡ç®—ä¸€è‡´æ€§æŸå¤±
            # å¯¹ Teacher åš Sharpening (å¯é€‰ï¼Œä½†æ¨è)
            teacher_prob = torch.softmax(pred_u_teacher, dim=1)
            student_prob = torch.softmax(pred_u_student, dim=1)
            
            loss_cons = loss_consistency(student_prob, teacher_prob)
            total_loss_cons = consistency_weight * loss_cons

            # åå‘ä¼ æ’­äº§ç”Ÿ g_U
            total_loss_cons.backward()
            optimizer.step_unlabeled()

            # -------------------------------------------------------
            # [æ­¥éª¤ 3] Teacher EMA æ›´æ–°
            # -------------------------------------------------------
            update_ema_variables(model, ema_model, EMA_DECAY, epoch * 100 + step)
            
            loss_sup_sum += loss_sup.item()
            loss_cons_sum += loss_cons.item()

        # æ—¥å¿—è®°å½•
        epoch_time = time.time() - epoch_start
        print(f"Ep {epoch+1}/{MAX_EPOCHS} | Time: {epoch_time:.0f}s | "
              f"L_Sup: {loss_sup_sum/max(step,1):.4f} | "
              f"L_Cons (w={consistency_weight:.3f}): {loss_cons_sum/max(step,1):.4f}", end="")

        # --- Validation (ä½¿ç”¨ Teacher è¯„ä¼°) ---
        if (epoch + 1) % VAL_INTERVAL == 0:
            ema_model.eval()
            with torch.no_grad():
                for val_data in loader_val:
                    val_in, val_lbl = val_data["image"].to(device), val_data["label"].to(device)
                    # æ»‘åŠ¨çª—å£æ¨ç†
                    val_out = sliding_window_inference(val_in, ROI_SIZE, 4, ema_model)
                    
                    val_out = [AsDiscrete(argmax=True, to_onehot=2)(i) for i in decollate_batch(val_out)]
                    val_lbl = [AsDiscrete(to_onehot=2)(i) for i in decollate_batch(val_lbl)]
                    
                    dice_metric(y_pred=val_out, y=val_lbl)
                
                metric = dice_metric.aggregate().item()
                dice_metric.reset()
                
                print(f" | Val Dice: {metric:.4f}", end="")
                
                if metric > best_metric:
                    best_metric = metric
                    # torch.save(ema_model.state_dict(), os.path.join(MODEL_SAVE_DIR, "best_teacher_tmo.pth"))
                    # torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, "best_student_tmo.pth"))
                    print(f" -> ğŸ”¥ Saved!", end="")

        print("")

    print(f"è®­ç»ƒç»“æŸã€‚Best Dice: {best_metric:.4f}")

if __name__ == "__main__":
    try:
        train_tmo()
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()