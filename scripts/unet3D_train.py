import sys
import os
import glob
import torch
import time
import warnings

# [æ–°å¢] å¿½ç•¥æ¥è‡ª MONAI/PyTorch çš„ç‰¹å®šæœªæ¥è­¦å‘Šï¼Œä¿æŒæ—¥å¿—å¹²å‡€
warnings.filterwarnings("ignore", category=UserWarning, module="monai.inferers.utils")

# --- è·¯å¾„é…ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.inferers import sliding_window_inference
from monai.utils import set_determinism
from monai.data import decollate_batch, partition_dataset
from monai.transforms import AsDiscrete

# å¯¼å…¥ä½ çš„æ¨¡å—
from src.models.unet3D import UNet3D
from src.dataloaders.basic_loader import get_basic_loader

def train_baseline():
    # ================= é…ç½®åŒºåŸŸ =================
    # GPUé…ç½® - æŒ‡å®šä½¿ç”¨å“ªå¼ æ˜¾å¡
    GPU_ID = "0"
    os.environ["CUDA_VISIBLE_DEVICES"] = GPU_ID
    print(f"ä½¿ç”¨GPU: {GPU_ID}")
    
    # è·¯å¾„é…ç½®
    DATA_DIR = "/home/ta/lzf/Code/dataset/nnUNet_raw/Dataset701_STS3D_ROI"  # ä½ çš„ ROI æ•°æ®è·¯å¾„
    MODEL_SAVE_DIR = "./weights"
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
    
    # è®­ç»ƒè¶…å‚æ•°ï¼ˆåŸºäºiterationï¼‰
    MAX_ITERATIONS = 3600  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    VAL_INTERVAL = 90      # éªŒè¯é—´éš”
    
    # å®é™…é€å…¥ GPU çš„ Batch Size = LOAD_BATCH_SIZE * NUM_SAMPLES
    LOAD_BATCH_SIZE = 1     # æ¯æ¬¡ä»ç£ç›˜/ç¼“å­˜è¯»å–å¤šå°‘ä¸ª Volume (é™ä½ IO å‹åŠ›)
    NUM_SAMPLES = 32         # æ¯ä¸ª Volume åˆ‡å¤šå°‘ä¸ª Patch (æé«˜æ•°æ®åˆ©ç”¨ç‡)
    
    LR = 1e-4               # å­¦ä¹ ç‡
    ROI_SIZE = (96, 96, 96) # Patch å¤§å°
    
    # æ˜¾å­˜/å†…å­˜ä¼˜åŒ–é…ç½®
    NUM_WORKERS = 3
    CACHE_RATE = 1          # æ•°æ®ç¼“å­˜æ¯”ä¾‹ï¼ˆ1=å…¨éƒ¨ç¼“å­˜ï¼‰
    
    # ================= 1. æ•°æ®å‡†å¤‡ =================
    set_determinism(seed=2025) 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    print("æ­£åœ¨æ‰«æå¹¶åˆ’åˆ†æ•°æ®é›†...")
    images = sorted(glob.glob(os.path.join(DATA_DIR, "imagesTr", "*.nii.gz")))
    labels = sorted(glob.glob(os.path.join(DATA_DIR, "labelsTr", "*.nii.gz")))
    
    if not images:
        raise ValueError(f"é”™è¯¯ï¼šåœ¨ {DATA_DIR} ä¸­æœªæ‰¾åˆ°æ•°æ®ï¼")
        
    data_dicts = [{"image": img, "label": lbl} for img, lbl in zip(images, labels)]
    
    train_files, val_files = partition_dataset(
        data=data_dicts, 
        ratios=[0.8, 0.2], 
        shuffle=True, 
        seed=2025
    )
    
    print(f"  - æ€»æ•°æ®é‡: {len(data_dicts)}")
    print(f"  - è®­ç»ƒé›† (80%): {len(train_files)} ä¾‹")
    print(f"  - éªŒè¯é›† (20%): {len(val_files)} ä¾‹")

    # ================= 2. åˆ›å»ºåŠ è½½å™¨ =================
    train_loader = get_basic_loader(
        data_list=train_files,
        batch_size=LOAD_BATCH_SIZE, 
        roi_size=ROI_SIZE, 
        num_samples=NUM_SAMPLES, # [æ–°å¢] å¯ç”¨å¤šæ ·æœ¬é‡‡æ ·
        is_train=True, 
        num_workers=NUM_WORKERS,
        cache_rate=CACHE_RATE,
    )
    
    val_loader = get_basic_loader(
        data_list=val_files,
        batch_size=1,
        roi_size=ROI_SIZE, 
        is_train=False, 
        num_workers=NUM_WORKERS,
        cache_rate=CACHE_RATE,
    )

    # ================= 3. æ¨¡å‹ä¸ä¼˜åŒ–å™¨ =================
    model = UNet3D(in_channels=1, out_channels=2).to(device)
    loss_function = DiceCELoss(to_onehot_y=True, softmax=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    dice_metric = DiceMetric(include_background=False, reduction="mean")

    # ================= 4. è®­ç»ƒå¾ªç¯ =================
    best_metric = -1
    best_metric_iteration = -1
    iteration = 0
    epoch_loss = 0
    step_in_epoch = 0
    
    print(f"\n{'='*20} å¼€å§‹è®­ç»ƒ (åŸºäºIteration) {'='*20}")
    print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {MAX_ITERATIONS}, éªŒè¯é—´éš”: {VAL_INTERVAL} iterations")
    
    model.train()
    train_iter = iter(train_loader)
    start_time = time.time() # è®°å½•æ€»å¼€å§‹æ—¶é—´
    loop_start_time = time.time() # è®°å½•å¾ªç¯å¼€å§‹æ—¶é—´
    
    while iteration < MAX_ITERATIONS:
        # è·å–ä¸‹ä¸€ä¸ªbatchï¼Œå¦‚æœæ•°æ®ç”¨å®Œåˆ™é‡æ–°å¼€å§‹
        try:
            batch_data = next(train_iter)
        except StopIteration:
            train_iter = iter(train_loader)
            batch_data = next(train_iter)
        
        iteration += 1
        step_in_epoch += 1
        
        inputs, labels_batch = batch_data["image"].to(device), batch_data["label"].to(device)

        optimizer.zero_grad()
        
        outputs = model(inputs)
        loss = loss_function(outputs, labels_batch)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        current_loss = loss.item()
        
        # è®¡ç®—å•æ¬¡è¿­ä»£æ—¶é—´
        current_time = time.time()
        iter_time = current_time - loop_start_time
        loop_start_time = current_time # é‡ç½®æ—¶é—´èµ·ç‚¹
        
        print(f"Iteration {iteration}/{MAX_ITERATIONS} | Time: {iter_time:.4f}s | Loss: {current_loss:.4f}")

        # --- Validation ---
        if iteration % VAL_INTERVAL == 0:
            # [ä¼˜åŒ–] éªŒè¯å‰æ¸…ç†æ˜¾å­˜ï¼Œä¸ºéªŒè¯é˜¶æ®µè…¾å‡ºç©ºé—´
            torch.cuda.empty_cache()
            
            model.eval()
            with torch.no_grad():
                for val_data in val_loader:
                    val_inputs, val_labels = val_data["image"].to(device), val_data["label"].to(device)
                    
                    val_outputs = sliding_window_inference(
                        inputs=val_inputs, 
                        roi_size=ROI_SIZE, 
                        sw_batch_size=4, 
                        predictor=model
                    )
                    
                    val_outputs = [AsDiscrete(argmax=True, to_onehot=2)(i) for i in decollate_batch(val_outputs)]
                    val_labels = [AsDiscrete(to_onehot=2)(i) for i in decollate_batch(val_labels)]
                    
                    dice_metric(y_pred=val_outputs, y=val_labels)

                metric = dice_metric.aggregate().item()
                dice_metric.reset()

                print(f"Validation at Iter {iteration} | Val Dice: {metric:.4f}", end="")

                if metric > best_metric:
                    best_metric = metric
                    best_metric_iteration = iteration
                    save_path = os.path.join(MODEL_SAVE_DIR, "best_unet3D_model.pth")
                    torch.save(model.state_dict(), save_path)
                    print(f" -> ğŸ”¥ New Best! ({best_metric:.4f})")
                else:
                    print("")

            # [ä¼˜åŒ–] éªŒè¯åæ¸…ç†æ˜¾å­˜ï¼Œé‡Šæ”¾éªŒè¯é˜¶æ®µçš„å¤§é‡å ç”¨ï¼Œä¸ºæ¥ä¸‹æ¥çš„è®­ç»ƒè…¾å‡ºç©ºé—´
            torch.cuda.empty_cache()
            
            model.train()
            # é‡ç½®ç»Ÿè®¡
            epoch_loss = 0
            step_in_epoch = 0

    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒç»“æŸã€‚æ€»ç”¨æ—¶: {total_time:.1f}s")
    print(f"æœ€ä½³æ¨¡å‹ Dice: {best_metric:.4f} äº Iteration {best_metric_iteration}")

if __name__ == "__main__":
    try:
        train_baseline()
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()