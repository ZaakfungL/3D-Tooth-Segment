import sys
import os
import glob
import torch
import json
import warnings
import numpy as np
import time
import warnings

# [æ–°å¢] å¿½ç•¥æ¥è‡ª MONAI/PyTorch çš„ç‰¹å®šæœªæ¥è­¦å‘Šï¼Œä¿æŒæ—¥å¿—å¹²å‡€
warnings.filterwarnings("ignore", category=UserWarning, module="monai.inferers.utils")


# --- è·¯å¾„é…ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.data import decollate_batch, partition_dataset
from monai.utils import set_determinism
from monai.transforms import AsDiscrete
from monai.inferers import sliding_window_inference

# å¯¼å…¥ä½ çš„æ¨¡å—
from src.models.dints import DiNTSWrapper
from src.dataloaders.basic_loader import get_basic_loader

def search_baseline():
    # ================= é…ç½®åŒºåŸŸ =================
    DATA_DIR = "/home/lzf/Code/dataset/nnUNet_raw/Dataset701_STS3D_ROI"
    MODEL_SAVE_DIR = "./weights"
    ARCH_SAVE_DIR = "./results/dints_arch" 
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
    os.makedirs(ARCH_SAVE_DIR, exist_ok=True)

    # [Debug ä¿®æ”¹] æœç´¢è¶…å‚æ•° - å¿«é€ŸéªŒè¯æ¨¡å¼
    MAX_EPOCHS = 5         
    VAL_INTERVAL = 1       
    
    # âš ï¸ æ˜¾å­˜ä¼˜åŒ–å…³é”®ç‚¹ï¼š
    # è™½ç„¶è¿™é‡Œè®¾ä¸º 1ï¼Œä½†ç”±äº basic_loader é‡Œ num_samples=2ï¼Œå®é™… Batch æ˜¯ 2
    BATCH_SIZE = 1         
    ROI_SIZE = (64, 64, 64)
    
    # å­¦ä¹ ç‡é…ç½®
    LR_WEIGHTS = 0.025     
    LR_ARCH = 3e-4         
    
    # èµ„æºé…ç½®
    NUM_WORKERS = 2
    CACHE_RATE = 0.0       

    # ================= 1. æ•°æ®å‡†å¤‡ (åŒå±‚åˆ’åˆ†) =================
    set_determinism(seed=2025)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ å¼€å§‹ DiNTS æœç´¢ (æç®€æ¨¡å¼) | è®¾å¤‡: {device}")

    images = sorted(glob.glob(os.path.join(DATA_DIR, "imagesTr", "*.nii.gz")))
    labels = sorted(glob.glob(os.path.join(DATA_DIR, "labelsTr", "*.nii.gz")))
    data_dicts = [{"image": i, "label": l} for i, l in zip(images, labels)]

    # NAS æ•°æ®åˆ’åˆ†
    train_files_all, val_files = partition_dataset(
        data=data_dicts, ratios=[0.8, 0.2], shuffle=True, seed=2025
    )
    
    train_files_w, train_files_a = partition_dataset(
        data=train_files_all, ratios=[0.5, 0.5], shuffle=True, seed=2025
    )

    print(f"  - æ€»æ•°æ®: {len(data_dicts)}")
    print(f"  - æƒé‡æ›´æ–°é›† (Train_W): {len(train_files_w)}")
    print(f"  - æ¶æ„æ›´æ–°é›† (Train_A): {len(train_files_a)}")
    print(f"  - éªŒè¯é›† (Val): {len(val_files)}")

    # [Debug] é™åˆ¶æ•°æ®é‡
    print("âš¡ æ­£åœ¨åˆ›å»ºåŠ è½½å™¨ (é™åˆ¶æ•°æ®é‡ä¸º 2)...")
    
    train_loader_w = get_basic_loader(
        data_list=train_files_w, 
        batch_size=BATCH_SIZE, 
        roi_size=ROI_SIZE, 
        num_samples=1,
        is_train=True, 
        num_workers=NUM_WORKERS, 
        cache_rate=CACHE_RATE,
        limit=1
    )
    
    train_loader_a = get_basic_loader(
        data_list=train_files_a, 
        batch_size=BATCH_SIZE, 
        roi_size=ROI_SIZE, 
        num_samples=1,
        is_train=True, 
        num_workers=NUM_WORKERS, 
        cache_rate=CACHE_RATE,
        limit=1
    )
    
    val_loader = get_basic_loader(
        data_list=val_files, 
        batch_size=1, 
        roi_size=ROI_SIZE, 
        num_samples=1,
        is_train=False, 
        num_workers=NUM_WORKERS, 
        cache_rate=CACHE_RATE,
        limit=1
    )

    # ================= 2. æ¨¡å‹ä¸åŒä¼˜åŒ–å™¨ =================
    # [æ˜¾å­˜ä¼˜åŒ–] å¤§å¹…å‰Šå‡é€šé“æ•°å’Œå±‚æ•°ï¼Œç¡®ä¿ 8G æ˜¾å­˜èƒ½è·‘é€š
    print("ğŸ”§ åˆå§‹åŒ– DiNTS æ¨¡å‹ (channel_mul=0.25, num_blocks=4)...")
    model = DiNTSWrapper(
        in_channels=1, 
        out_channels=2, 
        num_blocks=4,      # [ä¿®æ”¹] ä» 6 é™åˆ° 4
        num_depths=3,
        channel_mul=0.25,  # [ä¿®æ”¹] ä» 0.5 é™åˆ° 0.25 (é€šé“æ•°å‡åŠ)
        use_downsample=True 
    ).to(device)

    # [å…³é”®ä¿®å¤] æ˜¾å¼åŒæ­¥ TopologySearch å†…éƒ¨çš„ device å±æ€§
    if hasattr(model, "dints_space"):
        model.dints_space.device = device

    optimizer_w = torch.optim.SGD(
        model.weight_parameters(), 
        lr=LR_WEIGHTS, 
        momentum=0.9, 
        weight_decay=3e-4
    )
    
    optimizer_a = torch.optim.Adam(
        model.arch_parameters(), 
        lr=LR_ARCH, 
        betas=(0.5, 0.999), 
        weight_decay=0
    )

    loss_function = DiceCELoss(to_onehot_y=True, softmax=True)
    dice_metric = DiceMetric(include_background=False, reduction="mean")

    # ================= 3. æœç´¢å¾ªç¯ =================
    best_metric = -1
    best_metric_epoch = -1
    
    print(f"\n{'='*20} å¼€å§‹æœç´¢ {'='*20}")

    for epoch in range(MAX_EPOCHS):
        epoch_start = time.time()
        model.train()
        loss_w_sum = 0
        loss_a_sum = 0
        step = 0
        
        # ä½¿ç”¨ zip åŒæ—¶éå†ä¸¤ä¸ªåŠ è½½å™¨
        for batch_w, batch_a in zip(train_loader_w, train_loader_a):
            step += 1
            
            input_w, label_w = batch_w["image"].to(device), batch_w["label"].to(device)
            input_a, label_a = batch_a["image"].to(device), batch_a["label"].to(device)

            # ------------------------------------------------
            # é˜¶æ®µ A: æ›´æ–°æ¶æ„å‚æ•° (Alphas)
            # ------------------------------------------------
            optimizer_a.zero_grad()
            output_a = model(input_a)
            loss_a = loss_function(output_a, label_a)
            
            # [Fix æ ¸å¿ƒä¿®å¤: IndexKernel Error] 
            probs_children, _ = model.dints_space.get_prob_a(child=True)
            entropy_loss = model.dints_space.get_topology_entropy(probs_children)
            
            total_loss_a = loss_a + 0.001 * entropy_loss 

            total_loss_a.backward()
            optimizer_a.step()
            loss_a_sum += total_loss_a.item()

            # ------------------------------------------------
            # é˜¶æ®µ B: æ›´æ–°æƒé‡å‚æ•° (Weights)
            # ------------------------------------------------
            optimizer_w.zero_grad()
            output_w = model(input_w)
            loss_w = loss_function(output_w, label_w)
            
            loss_w.backward()
            optimizer_w.step()
            loss_w_sum += loss_w.item()

        epoch_time = time.time() - epoch_start
        print(f"Epoch {epoch+1}/{MAX_EPOCHS} | Time: {epoch_time:.1f}s | "
              f"Loss W: {loss_w_sum/max(step,1):.4f} | Loss A: {loss_a_sum/max(step,1):.4f}", end="")

        # --- éªŒè¯ä¸ä¿å­˜ ---
        if (epoch + 1) % VAL_INTERVAL == 0:
            model.eval()
            with torch.no_grad():
                for val_data in val_loader:
                    val_in, val_lbl = val_data["image"].to(device), val_data["label"].to(device)
                    val_pred = sliding_window_inference(val_in, ROI_SIZE, 4, model)
                    
                    val_pred = [AsDiscrete(argmax=True, to_onehot=2)(i) for i in decollate_batch(val_pred)]
                    val_lbl = [AsDiscrete(to_onehot=2)(i) for i in decollate_batch(val_lbl)]
                    dice_metric(y_pred=val_pred, y=val_lbl)
                
                metric = dice_metric.aggregate().item()
                dice_metric.reset()
                
                print(f" | Val Dice: {metric:.4f}", end="")
                
                if metric > best_metric:
                    best_metric = metric
                    best_metric_epoch = epoch + 1
                    try:
                        # è·å–æœ€ä½³æ¶æ„
                        topology = model.get_topology()
                        
                        arch_json = {
                            "arch_code_a": topology[1].tolist(),
                            "arch_code_c": topology[2].tolist()
                        }
                        save_path = os.path.join(ARCH_SAVE_DIR, "best_arch_roi.json")
                        with open(save_path, "w") as f:
                            json.dump(arch_json, f, indent=4)
                        
                        # torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, "dints_search_best.pth"))
                        print(f" -> ğŸ”¥ New Best! Saved arch", end="")
                    except Exception as e:
                        print(f" -> [Err] Save Failed: {e}", end="")

        print("") 

    print(f"\næœç´¢ç»“æŸã€‚æœ€ä½³æ¶æ„ Dice: {best_metric:.4f}")

if __name__ == "__main__":
    try:
        search_baseline()
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()