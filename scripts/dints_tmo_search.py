import os
import sys
import glob
import torch
import numpy as np
import time
import warnings
import json
import argparse

# è¿‡æ»¤ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="monai.inferers.utils")

# --- è·¯å¾„é…ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from monai.config import print_config
from monai.utils import set_determinism
from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.data import decollate_batch
from monai.inferers import sliding_window_inference
from monai.transforms import AsDiscrete

from src.dataloaders.combo_loader import NASComboDataLoader
from src.models.dints import DiNTSWrapper
from src.ssl.tmo import TMOAdamW
from src.ssl.utils import update_ema_variables, ConsistencyLoss, get_current_consistency_weight
from src.utils.config import load_config, get_config_argument_parser

def search_tmo(config):
    # ================= é…ç½®è¯»å– (Fail Fast) =================
    # åŸºç¡€é…ç½®
    data_dir = config["data_dir"]
    log_dir = config["log_dir"]
    roi_size = tuple(config["roi_size"])
    num_workers = config["num_workers"]
    cache_rate = config["cache_rate"]

    # è®­ç»ƒå‚æ•°
    max_epochs = config["max_epochs"]
    batch_size = config["batch_size"]
    val_freq = config["val_freq"]
    arch_start_epoch = config["arch_start_epoch"]
    unlabeled_ratio = config["unlabeled_ratio"]

    # ä¼˜åŒ–å™¨ä¸æŸå¤±
    lr_weights = config["lr_weights"]
    lr_arch = config["lr_arch"]
    ema_alpha = config["ema_alpha"]
    consistency = config["consistency"]
    consistency_rampup = config["consistency_rampup"]

    seed = config.get("seed", 2025)

    # 0. è®¾ç½®
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_determinism(0) # TMO script used 0 in original code, keeping it or using seed? Original used 0.
    # Actually, let's use the seed from config if provided, or default to 0 if that's what was intended.
    # Original code had `set_determinism(0)` hardcoded. Let's use `seed`.
    set_determinism(seed)

    print(f"ğŸš€ å¼€å§‹ DiNTS-TMO æœç´¢ | è®¾å¤‡: {device} | Seed: {seed}")
    print(f"ğŸ“‚ æ•°æ®é›†: {data_dir}")

    # 1. æ¨¡å‹åˆå§‹åŒ–
    print("åˆå§‹åŒ–æ¨¡å‹...")
    # Student: éœ€è¦æ¢¯åº¦
    student = DiNTSWrapper(
        in_channels=1,
        out_channels=2,
        num_blocks=6,
        num_depths=3
    ).to(device)

    # Teacher: ä¸éœ€è¦æ¢¯åº¦ï¼Œä½¿ç”¨ EMA æƒé‡
    teacher = DiNTSWrapper(
        in_channels=1,
        out_channels=2,
        num_blocks=6,
        num_depths=3
    ).to(device)

    # å¦‚æœéœ€è¦ï¼Œæ˜¾å¼è®¾ç½® dints_space çš„è®¾å¤‡
    if hasattr(student, "dints_space"):
        student.dints_space.device = device
    if hasattr(teacher, "dints_space"):
        teacher.dints_space.device = device

    # åˆ†ç¦» Teacher å‚æ•°
    for p in teacher.parameters():
        p.detach_()

    # 2. æ•°æ®åŠ è½½å™¨
    print("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")

    # ===== å…ˆåˆ’åˆ† Train/Valï¼Œé¿å…æ•°æ®æ³„æ¼ =====
    from monai.data import partition_dataset
    from src.dataloaders.basic_loader import get_basic_loader

    # æ‰«ææœ‰æ ‡ç­¾æ•°æ®
    images_l = sorted(glob.glob(os.path.join(data_dir, "imagesTr", "*.nii.gz")))
    labels_l = sorted(glob.glob(os.path.join(data_dir, "labelsTr", "*.nii.gz")))

    if not images_l:
         raise ValueError(f"æœªåœ¨ {data_dir} æ‰¾åˆ°æœ‰æ ‡ç­¾æ•°æ®ï¼")

    all_labeled = [{"image": i, "label": l} for i, l in zip(images_l, labels_l)]

    # åˆ’åˆ† Train/Val (80%/20%)ï¼Œä¿è¯äº’æ–¥
    train_labeled, val_labeled = partition_dataset(
        data=all_labeled, ratios=[0.8, 0.2], shuffle=True, seed=seed
    )
    print(f"ğŸ“Š Train/Val åˆ’åˆ†: Train {len(train_labeled)} ä¾‹ | Val {len(val_labeled)} ä¾‹")

    # æ‰«ææ— æ ‡ç­¾æ•°æ® (ä¸éœ€è¦åˆ’åˆ†ï¼Œå…¨éƒ¨ç”¨äºè®­ç»ƒ)
    images_u = sorted(glob.glob(os.path.join(data_dir, "imagesUnlabeled", "*.nii.gz")))

    if not images_u:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æ— æ ‡ç­¾æ•°æ® (imagesUnlabeled)ï¼Œå°†ä»…ä½¿ç”¨æœ‰æ ‡ç­¾æ•°æ®ã€‚")
        all_unlabeled = []
    else:
        all_unlabeled = [{"image": i, "label": i} for i in images_u]  # label å ä½ï¼Œä¸ä½¿ç”¨

        # ===== æ§åˆ¶æœ‰æ ‡ç­¾/æ— æ ‡ç­¾æ¯”ä¾‹ =====
        # æŒ‰ 1:1 æ¯”ä¾‹æˆªå–æ— æ ‡ç­¾æ•°æ®ï¼Œé¿å…åŠ è½½è¿‡å¤šæ•°æ®æµªè´¹ RAM
        # unlabeled_ratio (1.0 = 1:1)
        max_unlabeled = int(len(train_labeled) * unlabeled_ratio)
        if len(all_unlabeled) > max_unlabeled:
            # éšæœºé‡‡æ ·ï¼Œä¿è¯å¤šæ ·æ€§
            import random
            random.seed(seed)
            all_unlabeled = random.sample(all_unlabeled, max_unlabeled)
        print(f"ğŸ“Š æ— æ ‡ç­¾æ•°æ®: {len(all_unlabeled)} ä¾‹ (æ¯”ä¾‹ 1:{unlabeled_ratio})")

    # ===== åˆ›å»º NASComboDataLoader (åªç”¨è®­ç»ƒæ•°æ®) =====
    combo_loader = NASComboDataLoader(
        labeled_list=train_labeled,      # ä¼ å…¥åˆ’åˆ†å¥½çš„æœ‰æ ‡ç­¾è®­ç»ƒé›†
        unlabeled_list=all_unlabeled,    # ä¼ å…¥å…¨éƒ¨æ— æ ‡ç­¾æ•°æ®
        batch_size_l=batch_size,
        batch_size_u=batch_size,
        roi_size=roi_size,
        num_workers=num_workers,
        cache_rate=cache_rate,
        # limit=4  # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š
    )

    # ===== åˆ›å»ºéªŒè¯åŠ è½½å™¨ =====
    val_loader = get_basic_loader(
        data_list=val_labeled,
        batch_size=1,
        roi_size=roi_size,
        is_train=False,
        num_workers=num_workers,
        cache_rate=cache_rate
    )

    # 3. ä¼˜åŒ–å™¨åˆå§‹åŒ–
    print("åˆå§‹åŒ–ä¼˜åŒ–å™¨ (TMO)...")
    optimizer_w = TMOAdamW(student.weight_parameters(), lr=lr_weights, weight_decay=1e-4)
    optimizer_a = TMOAdamW(student.arch_parameters(), lr=lr_arch, weight_decay=1e-3)

    # 4. æŸå¤±å‡½æ•°
    loss_dice_ce = DiceCELoss(to_onehot_y=True, softmax=True, batch=True)
    loss_consistency = ConsistencyLoss()
    dice_metric = DiceMetric(include_background=False, reduction="mean")

    # --- è®­ç»ƒå¾ªç¯ ---
    print(f"\n{'='*20} å¼€å§‹æœç´¢å¾ªç¯ ({max_epochs} epochs) {'='*20}")
    global_step = 0
    best_metric = -1

    for epoch in range(max_epochs):
        epoch_start = time.time()
        student.train()
        teacher.train()

        loss_w_l_sum = 0
        loss_w_u_sum = 0
        loss_a_l_sum = 0
        loss_a_u_sum = 0
        step = 0

        cons_weight = get_current_consistency_weight(epoch, max_epochs, consistency, consistency_rampup)

        for batch_data in combo_loader:
            step += 1
            global_step += 1

            l_w_imgs, l_w_lbls = batch_data['l_w']['image'].to(device), batch_data['l_w']['label'].to(device)
            u_w_imgs = batch_data['u_w']['image'].to(device)
            l_a_imgs, l_a_lbls = batch_data['l_a']['image'].to(device), batch_data['l_a']['label'].to(device)
            u_a_imgs = batch_data['u_a']['image'].to(device)

            # --- é˜¶æ®µ A: ä¼˜åŒ–æ¶æ„å‚æ•° (Alpha) ---
            if epoch >= arch_start_epoch:
                # A1. æœ‰æ ‡ç­¾æ­¥éª¤ (Labeled Step)
                optimizer_a.zero_grad()
                outputs_l_a = student(l_a_imgs)
                loss_a_l = loss_dice_ce(outputs_l_a, l_a_lbls)

                # ç†µæŸå¤± (å¯é€‰ï¼Œæ¨¡ä»¿ dints_search çš„è¡Œä¸º)
                probs_children, _ = student.dints_space.get_prob_a(child=True)
                entropy_loss = student.dints_space.get_topology_entropy(probs_children)
                loss_a_l_total = loss_a_l + 0.001 * entropy_loss

                loss_a_l_total.backward()
                optimizer_a.step_labeled()
                loss_a_l_sum += loss_a_l.item()

                # A2. æ— æ ‡ç­¾æ­¥éª¤ (Unlabeled Step)
                optimizer_a.zero_grad()

                # åŒæ­¥ Teacher æ¶æ„
                with torch.no_grad():
                    teacher.dints_space.log_alpha_a.copy_(student.dints_space.log_alpha_a)
                    teacher.dints_space.log_alpha_c.copy_(student.dints_space.log_alpha_c)

                outputs_u_a = student(u_a_imgs)
                with torch.no_grad():
                    teacher_u_a = teacher(u_a_imgs)
                    teacher_u_a = torch.softmax(teacher_u_a, dim=1)

                student_u_a_soft = torch.softmax(outputs_u_a, dim=1)
                loss_a_u = loss_consistency(student_u_a_soft, teacher_u_a) * cons_weight

                loss_a_u.backward()
                optimizer_a.step_unlabeled()
                loss_a_u_sum += loss_a_u.item()

            # --- é˜¶æ®µ B: ä¼˜åŒ–æƒé‡å‚æ•° (Weights) ---
            # B1. æœ‰æ ‡ç­¾æ­¥éª¤ (Labeled Step)
            optimizer_w.zero_grad()
            outputs_l_w = student(l_w_imgs)
            loss_w_l = loss_dice_ce(outputs_l_w, l_w_lbls)

            loss_w_l.backward()
            optimizer_w.step_labeled()
            loss_w_l_sum += loss_w_l.item()

            # B2. æ— æ ‡ç­¾æ­¥éª¤ (Unlabeled Step)
            optimizer_w.zero_grad()

            # åŒæ­¥ Teacher æ¶æ„åˆ°æœ€æ–°çŠ¶æ€ (ç¡®ä¿ Teacher ä½¿ç”¨æœ€æ–°çš„æ¶æ„å‚æ•°)
            with torch.no_grad():
                teacher.dints_space.log_alpha_a.copy_(student.dints_space.log_alpha_a)
                teacher.dints_space.log_alpha_c.copy_(student.dints_space.log_alpha_c)

            outputs_u_w = student(u_w_imgs)
            with torch.no_grad():
                teacher_u_w = teacher(u_w_imgs)
                teacher_u_w = torch.softmax(teacher_u_w, dim=1)

            student_u_w_soft = torch.softmax(outputs_u_w, dim=1)
            loss_w_u = loss_consistency(student_u_w_soft, teacher_u_w) * cons_weight

            loss_w_u.backward()
            optimizer_w.step_unlabeled()
            loss_w_u_sum += loss_w_u.item()

            # --- é˜¶æ®µ C: ç»´æŠ¤ Teacher æ¨¡å‹ ---
            # C1. EMA æ›´æ–°æƒé‡å‚æ•°
            update_ema_variables(student, teacher, ema_alpha, global_step)

            # C2. åŒæ­¥æ¶æ„å‚æ•° (æœ‰ä¸¤ç§ç­–ç•¥ï¼Œæ ¹æ® Algorithm 3 ç¬¬10è¡Œ)
            # ç­–ç•¥ A: ç›´æ¥å¤ç”¨ Student çš„æœ€æ–°æ¶æ„ (æ¨èï¼Œæ›´ç®€å•)
            # ç­–ç•¥ B: å¯¹æ¶æ„å‚æ•°ä¹Ÿåš EMA (å¯¹åº”å›¾ç‰‡ç®—æ³•)
            # è¿™é‡Œé‡‡ç”¨ç­–ç•¥ Aï¼Œå› ä¸ºæ¶æ„å‚æ•°å˜åŒ–è¾ƒæ…¢ï¼Œç›´æ¥åŒæ­¥æ›´ç¨³å®š
            with torch.no_grad():
                teacher.dints_space.log_alpha_a.copy_(student.dints_space.log_alpha_a)
                teacher.dints_space.log_alpha_c.copy_(student.dints_space.log_alpha_c)

        # Epoch ç»“æŸæ—¥å¿—
        epoch_time = time.time() - epoch_start
        print(f"Ep {epoch+1}/{max_epochs} | Time: {epoch_time:.1f}s | "
              f"L_W(L): {loss_w_l_sum/max(step,1):.4f} L_W(U): {loss_w_u_sum/max(step,1):.4f} | "
              f"L_A(L): {loss_a_l_sum/max(step,1):.4f} L_A(U): {loss_a_u_sum/max(step,1):.4f}", end="")

        # éªŒè¯
        if (epoch + 1) % val_freq == 0:
            teacher.eval() # ä½¿ç”¨ Teacher è¿›è¡ŒéªŒè¯
            with torch.no_grad():
                for val_data in val_loader:
                    val_in, val_lbl = val_data["image"].to(device), val_data["label"].to(device)
                    val_pred = sliding_window_inference(val_in, roi_size, 4, teacher)
                    val_pred = [AsDiscrete(argmax=True, to_onehot=2)(i) for i in decollate_batch(val_pred)]
                    val_lbl = [AsDiscrete(to_onehot=2)(i) for i in decollate_batch(val_lbl)]
                    dice_metric(y_pred=val_pred, y=val_lbl)

                metric = dice_metric.aggregate().item()
                dice_metric.reset()

                print(f" | Val Dice: {metric:.4f}", end="")

                if metric > best_metric:
                    best_metric = metric
                    # ä¿å­˜æœ€ä½³ç»“æœ
                    topology = teacher.get_topology()
                    arch_json = {"arch_code_a": topology[1].tolist(), "arch_code_c": topology[2].tolist()}
                    with open(os.path.join(log_dir, "best_arch.json"), "w") as f:
                        json.dump(arch_json, f)
                    torch.save(teacher.state_dict(), os.path.join(log_dir, "model_best.pth"))
                    print(f" -> ğŸ”¥ New Best!", end="")

        print("")

    print(f"\næœç´¢ç»“æŸã€‚æœ€ä½³ Dice: {best_metric:.4f}")

if __name__ == "__main__":
    parser = get_config_argument_parser(description="DiNTS TMO Search Script")
    parser.add_argument("--seed", type=int, default=2025, help="Random seed (default: 2025)")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    default_config_path = os.path.join(project_root, "configs", "dints_tmo_search.yaml")
    config_path = args.config if args.config else default_config_path

    config = load_config(config_path, default_config=None)

    if not config:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}ï¼Œæˆ–æ–‡ä»¶ä¸ºç©ºï¼")
        sys.exit(1)

    # ä¼˜å…ˆçº§: Args > Config > Default
    if args.seed != 2025:
        config["seed"] = args.seed
    elif "seed" not in config:
        config["seed"] = 2025

    try:
        search_tmo(config)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
