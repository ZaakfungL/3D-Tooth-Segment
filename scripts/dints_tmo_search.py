import os
import sys
import glob
import torch
import numpy as np
import time
import warnings
import json

# è¿‡æ»¤ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="monai.inferers.utils")

# --- è·¯å¾„é…ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from monai.config import print_config
from monai.utils import set_determinism
from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.data import decollate_batch
from monai.inferers import sliding_window_inference
from monai.transforms import AsDiscrete

from src.dataloaders.combo_loader import NASComboDataLoader
from src.models.dints import DiNTSWrapper
from src.ssl.tmo import TMOAdamW
from src.ssl.utils import update_ema_variables, ConsistencyLoss, get_current_consistency_weight

# --- é…ç½® (ç›®å‰ä¸ºç¡¬ç¼–ç ï¼Œç”¨äºéªŒè¯) ---
DATA_DIR = "/home/lzf/Code/dataset/nnUNet_raw/Dataset701_STS3D_ROI"
LOG_DIR = "./results/dints_tmo_search"
MAX_EPOCHS = 2
BATCH_SIZE = 2
LR_WEIGHTS = 0.025
LR_ARCH = 0.003
VAL_FREQ = 1
ARCH_START_EPOCH = 0
EMA_ALPHA = 0.99
CONSISTENCY = 10.0
CONSISTENCY_RAMPUP = 50.0
ROI_SIZE = (64, 64, 64) # æ ¹æ®ç”¨æˆ·è¦æ±‚æ›´æ–°ä¸º 64

def search_tmo():
    # 0. è®¾ç½®
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_determinism(0)
    
    print(f"ğŸš€ å¼€å§‹ DiNTS-TMO æœç´¢ | è®¾å¤‡: {device}")

    # 1. æ¨¡å‹åˆå§‹åŒ–
    print("åˆå§‹åŒ–æ¨¡å‹...")
    # Student: éœ€è¦æ¢¯åº¦
    student = DiNTSWrapper(
        in_channels=1, 
        out_channels=2, 
        num_blocks=6, 
        num_depths=3
    ).to(device)
    
    # Teacher: ä¸éœ€è¦æ¢¯åº¦ï¼Œä½¿ç”¨ EMA æƒé‡
    teacher = DiNTSWrapper(
        in_channels=1, 
        out_channels=2, 
        num_blocks=6, 
        num_depths=3
    ).to(device)
    
    # å¦‚æœéœ€è¦ï¼Œæ˜¾å¼è®¾ç½® dints_space çš„è®¾å¤‡
    if hasattr(student, "dints_space"):
        student.dints_space.device = device
    if hasattr(teacher, "dints_space"):
        teacher.dints_space.device = device
    
    # åˆ†ç¦» Teacher å‚æ•°
    for p in teacher.parameters():
        p.detach_()

    # 2. æ•°æ®åŠ è½½å™¨
    print("åˆå§‹åŒ– NASComboDataLoader (å››è·¯æ•°æ®æµ)...")
    combo_loader = NASComboDataLoader(
        data_dir=DATA_DIR,
        batch_size_l=BATCH_SIZE,
        batch_size_u=BATCH_SIZE,
        roi_size=ROI_SIZE,
        limit=4 # éªŒè¯æ¨¡å¼ä¸‹é™åˆ¶æ•°æ®é‡ä¸º 4
    )
    
    # éªŒè¯åŠ è½½å™¨ (æš‚æ—¶å¤ç”¨ basic_loader æˆ–åˆ›å»ºæ–°çš„)
    # é€šå¸¸åº”è¯¥ä½¿ç”¨ basic loader è¿›è¡ŒéªŒè¯ã€‚
    # å¤ç”¨ dints_search.py çš„éªŒè¯é€»è¾‘
    from src.dataloaders.basic_loader import get_basic_loader
    
    # ç®€å•çš„éªŒè¯é›†åˆ›å»º (ä¸ºäº†æœ¬æ¬¡æµ‹è¯•ï¼Œæš‚æ—¶æœ¬åœ°æ‹†åˆ†)
    # å®é™…ä¸Š NASCombo åº”è¯¥åªåŒ…å«è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨æœ¬åœ°æ‰«æå¹¶æ‹†åˆ†éªŒè¯é›†ã€‚
    # ä½¿ç”¨æœ‰æ ‡ç­¾æ•°æ®æ‹†åˆ†å‡ºä¸€å°éƒ¨åˆ†ä½œä¸ºä¼ªéªŒè¯é›†ã€‚
    images_l = sorted(glob.glob(os.path.join(DATA_DIR, "imagesTr", "*.nii.gz")))
    labels_l = sorted(glob.glob(os.path.join(DATA_DIR, "labelsTr", "*.nii.gz")))
    dicts_l = [{"image": i, "label": l} for i, l in zip(images_l, labels_l)]
    
    # å–æœ€å 20% ä½œä¸ºä¼ªéªŒè¯é›†
    val_split_idx = int(len(dicts_l) * 0.8)
    val_dicts = dicts_l[val_split_idx:]
    if len(val_dicts) == 0: val_dicts = dicts_l # å›é€€ç­–ç•¥
    
    val_loader = get_basic_loader(
        data_list=val_dicts[:2], # é™åˆ¶éªŒè¯é›†å¤§å°ä»¥åŠ å¿« dry run
        batch_size=1,
        roi_size=ROI_SIZE,
        is_train=False,
        num_workers=0
    )

    # 3. ä¼˜åŒ–å™¨åˆå§‹åŒ–
    print("åˆå§‹åŒ–ä¼˜åŒ–å™¨ (TMO)...")
    optimizer_w = TMOAdamW(student.weight_parameters(), lr=LR_WEIGHTS, weight_decay=1e-4)
    optimizer_a = TMOAdamW(student.arch_parameters(), lr=LR_ARCH, weight_decay=1e-3)

    # 4. æŸå¤±å‡½æ•°
    loss_dice_ce = DiceCELoss(to_onehot_y=True, softmax=True, batch=True)
    loss_consistency = ConsistencyLoss()
    dice_metric = DiceMetric(include_background=False, reduction="mean")

    # --- è®­ç»ƒå¾ªç¯ ---
    print(f"\n{'='*20} å¼€å§‹æœç´¢å¾ªç¯ ({MAX_EPOCHS} epochs) {'='*20}")
    global_step = 0
    best_metric = -1
    
    for epoch in range(MAX_EPOCHS):
        epoch_start = time.time()
        student.train()
        teacher.train() 
        
        loss_w_l_sum = 0
        loss_w_u_sum = 0
        loss_a_l_sum = 0
        loss_a_u_sum = 0
        step = 0
        
        cons_weight = get_current_consistency_weight(epoch, MAX_EPOCHS, CONSISTENCY, CONSISTENCY_RAMPUP)
        
        for batch_data in combo_loader:
            step += 1
            global_step += 1
            
            l_w_imgs, l_w_lbls = batch_data['l_w']['image'].to(device), batch_data['l_w']['label'].to(device)
            u_w_imgs = batch_data['u_w']['image'].to(device)
            l_a_imgs, l_a_lbls = batch_data['l_a']['image'].to(device), batch_data['l_a']['label'].to(device)
            u_a_imgs = batch_data['u_a']['image'].to(device)

            # --- é˜¶æ®µ A: ä¼˜åŒ–æ¶æ„å‚æ•° (Alpha) ---
            if epoch >= ARCH_START_EPOCH:
                # A1. æœ‰æ ‡ç­¾æ­¥éª¤ (Labeled Step)
                optimizer_a.zero_grad()
                outputs_l_a = student(l_a_imgs)
                loss_a_l = loss_dice_ce(outputs_l_a, l_a_lbls)
                
                # ç†µæŸå¤± (å¯é€‰ï¼Œæ¨¡ä»¿ dints_search çš„è¡Œä¸º)
                probs_children, _ = student.dints_space.get_prob_a(child=True)
                entropy_loss = student.dints_space.get_topology_entropy(probs_children)
                loss_a_l_total = loss_a_l + 0.001 * entropy_loss
                
                loss_a_l_total.backward()
                optimizer_a.step_labeled()
                loss_a_l_sum += loss_a_l.item()

                # A2. æ— æ ‡ç­¾æ­¥éª¤ (Unlabeled Step)
                optimizer_a.zero_grad()
                
                # åŒæ­¥ Teacher æ¶æ„
                with torch.no_grad():
                    teacher.dints_space.log_alpha_a.copy_(student.dints_space.log_alpha_a)
                    teacher.dints_space.log_alpha_c.copy_(student.dints_space.log_alpha_c)
                
                outputs_u_a = student(u_a_imgs)
                with torch.no_grad():
                    teacher_u_a = teacher(u_a_imgs)
                    teacher_u_a = torch.softmax(teacher_u_a, dim=1)
                
                student_u_a_soft = torch.softmax(outputs_u_a, dim=1)
                loss_a_u = loss_consistency(student_u_a_soft, teacher_u_a) * cons_weight
                
                loss_a_u.backward()
                optimizer_a.step_unlabeled()
                loss_a_u_sum += loss_a_u.item()
            
            # --- é˜¶æ®µ B: ä¼˜åŒ–æƒé‡å‚æ•° (Weights) ---
            # B1. æœ‰æ ‡ç­¾æ­¥éª¤ (Labeled Step)
            optimizer_w.zero_grad()
            outputs_l_w = student(l_w_imgs)
            loss_w_l = loss_dice_ce(outputs_l_w, l_w_lbls)
            
            loss_w_l.backward()
            optimizer_w.step_labeled()
            loss_w_l_sum += loss_w_l.item()
            
            # B2. æ— æ ‡ç­¾æ­¥éª¤ (Unlabeled Step)
            optimizer_w.zero_grad()
            outputs_u_w = student(u_w_imgs)
            with torch.no_grad():
                teacher_u_w = teacher(u_w_imgs)
                teacher_u_w = torch.softmax(teacher_u_w, dim=1)
            
            student_u_w_soft = torch.softmax(outputs_u_w, dim=1)
            loss_w_u = loss_consistency(student_u_w_soft, teacher_u_w) * cons_weight
            
            loss_w_u.backward()
            optimizer_w.step_unlabeled()
            loss_w_u_sum += loss_w_u.item()

            # --- é˜¶æ®µ C: ç»´æŠ¤ ---
            update_ema_variables(student, teacher, EMA_ALPHA, global_step)

        # Epoch ç»“æŸæ—¥å¿—
        epoch_time = time.time() - epoch_start
        print(f"Ep {epoch+1}/{MAX_EPOCHS} | Time: {epoch_time:.1f}s | "
              f"L_W(L): {loss_w_l_sum/max(step,1):.4f} L_W(U): {loss_w_u_sum/max(step,1):.4f} | "
              f"L_A(L): {loss_a_l_sum/max(step,1):.4f} L_A(U): {loss_a_u_sum/max(step,1):.4f}", end="")

        # éªŒè¯
        if (epoch + 1) % VAL_FREQ == 0:
            teacher.eval() # ä½¿ç”¨ Teacher è¿›è¡ŒéªŒè¯
            with torch.no_grad():
                for val_data in val_loader:
                    val_in, val_lbl = val_data["image"].to(device), val_data["label"].to(device)
                    val_pred = sliding_window_inference(val_in, ROI_SIZE, 4, teacher)
                    val_pred = [AsDiscrete(argmax=True, to_onehot=2)(i) for i in decollate_batch(val_pred)]
                    val_lbl = [AsDiscrete(to_onehot=2)(i) for i in decollate_batch(val_lbl)]
                    dice_metric(y_pred=val_pred, y=val_lbl)
                
                metric = dice_metric.aggregate().item()
                dice_metric.reset()
                
                print(f" | Val Dice: {metric:.4f}", end="")
                
                if metric > best_metric:
                    best_metric = metric
                    # ä¿å­˜æœ€ä½³ç»“æœ
                    topology = teacher.get_topology()
                    arch_json = {"arch_code_a": topology[1].tolist(), "arch_code_c": topology[2].tolist()}
                    with open(os.path.join(LOG_DIR, "best_arch.json"), "w") as f:
                        json.dump(arch_json, f)
                    torch.save(teacher.state_dict(), os.path.join(LOG_DIR, "model_best.pth"))
                    print(f" -> ğŸ”¥ New Best!", end="")
        
        print("")

    print(f"\næœç´¢ç»“æŸã€‚æœ€ä½³ Dice: {best_metric:.4f}")

if __name__ == "__main__":
    try:
        search_tmo()
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
