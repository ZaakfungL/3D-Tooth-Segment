import torch
import torch.nn as nn
from monai.utils import optional_import
from monai.networks.nets import DiNTS as MonaiDiNTS
from monai.networks.nets import TopologySearch

class DiNTSWrapper(nn.Module):
    """
    DiNTS æœç´¢ç©ºé—´çš„å°è£…å™¨ã€‚
    ç”¨äº NAS æœç´¢é˜¶æ®µ (Stage 1 & Stage 2 Search)ã€‚
    """
    def __init__(self, 
                 in_channels: int = 1, 
                 out_channels: int = 2, 
                 num_blocks: int = 6,       # [æ˜¾å­˜ä¼˜åŒ–] é»˜è®¤å‡å°ä¸º 6
                 num_depths: int = 3,       # [æ˜¾å­˜ä¼˜åŒ–] é»˜è®¤å‡å°ä¸º 3
                 use_downsample: bool = True,
                 spatial_dims: int = 3,
                 p_dropout: float = 0.1,
                 channel_mul: int = 1):
        super(DiNTSWrapper, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # 1. å®ä¾‹åŒ–æ‹“æ‰‘æœç´¢ç©ºé—´ (TopologySearch)
        self.dints_space = TopologySearch(
            spatial_dims=spatial_dims,
            num_blocks=num_blocks,
            num_depths=num_depths,
            use_downsample=use_downsample,
            channel_mul=channel_mul,
        )

        # 2. å®šä¹‰ DiNTS è¶…ç½‘ç»œ
        self.dints_search = MonaiDiNTS(
            dints_space=self.dints_space,
            in_channels=in_channels,
            num_classes=out_channels,
            act_name="RELU",
            norm_name="INSTANCE",
            use_downsample=use_downsample,
            spatial_dims=spatial_dims,
        )

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ã€‚
        æ³¨æ„ï¼šDiNTS å†…éƒ¨å·²ç»ç®¡ç†äº†æ¶æ„å‚æ•° (log_alpha_a, log_alpha_c) çš„ä½¿ç”¨ã€‚
        """
        return self.dints_search(x)

    def arch_parameters(self):
        """
        è¿”å›æ¶æ„å‚æ•° (Alphas)ï¼Œç”¨äºæ¶æ„ä¼˜åŒ–å™¨ (Arch Optimizer)ã€‚
        TopologySearch å†…éƒ¨æœ‰ä¸¤ä¸ªæ¶æ„å‚æ•°:
        - log_alpha_a: å®è§‚è·¯å¾„æƒé‡
        - log_alpha_c: å¾®è§‚æ“ä½œæƒé‡
        """
        return [self.dints_space.log_alpha_a, self.dints_space.log_alpha_c]

    def weight_parameters(self):
        """
        è¿”å›æƒé‡å‚æ•° (Weights)ï¼Œç”¨äºæƒé‡ä¼˜åŒ–å™¨ (Weight Optimizer)ã€‚
        æ’é™¤æ‰æ¶æ„å‚æ•°ã€‚
        """
        # æ”¶é›†æ‰€æœ‰å‚æ•°
        all_params = list(self.parameters())
        # æ”¶é›†æ¶æ„å‚æ•°çš„ ID
        arch_ids = list(map(id, self.arch_parameters()))
        # è¿‡æ»¤
        return [p for p in all_params if id(p) not in arch_ids]

    def get_topology(self):
        """
        è§£ç æœ€ç»ˆæ¶æ„ã€‚
        """
        # TopologySearch æä¾›äº† decode æ–¹æ³•ï¼Œè¿”å› (node_a, arch_code_a, arch_code_c, arch_code_a_max)
        return self.dints_space.decode()

# ==========================================
# å•å…ƒæµ‹è¯•ä»£ç 
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨æµ‹è¯• DiNTSWrapper (Search Mode)...")
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("ä½¿ç”¨è®¾å¤‡: CUDA")
    else:
        device = torch.device("cpu")
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    
    try:
        # 1. åˆå§‹åŒ–æ¨¡å‹
        model = DiNTSWrapper(
            in_channels=1, 
            out_channels=2, 
            num_blocks=6, 
            num_depths=3
        ).to(device)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æ£€æŸ¥æ¶æ„å‚æ•°
        arch_params = model.arch_parameters()
        print(f"æ¶æ„å‚æ•°æ•°é‡: {len(arch_params)} (åº”ä¸º 2: log_alpha_a, log_alpha_c)")
        print(f"log_alpha_a shape: {arch_params[0].shape}")
        print(f"log_alpha_c shape: {arch_params[1].shape}")
        
        # 3. å‰å‘ä¼ æ’­æµ‹è¯•
        # DiNTS çš„è¾“å…¥è¦æ±‚æ¯”è¾ƒç‰¹æ®Šï¼Œé€šå¸¸éœ€è¦ç¬¦åˆ 2^num_depths çš„å€æ•°
        input_tensor = torch.randn(2, 1, 64, 64, 64).to(device)
        output = model(input_tensor)
        print(f"è¾“å…¥ Shape: {input_tensor.shape}")
        print(f"è¾“å‡º Shape: {output.shape}") 
        
        if output.shape == (2, 2, 64, 64, 64):
            print("âœ… å‰å‘ä¼ æ’­å½¢çŠ¶åŒ¹é…")
        else:
            print(f"âŒ å‰å‘ä¼ æ’­å½¢çŠ¶é”™è¯¯: {output.shape}")

        # 4. åå‘ä¼ æ’­æµ‹è¯• (éªŒè¯ Alpha æ¢¯åº¦)
        loss = output.sum()
        loss.backward()
        
        # æ£€æŸ¥æ¶æ„å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
        if model.dints_space.log_alpha_a.grad is not None:
            print("âœ… æ¶æ„å‚æ•° (Alpha) æ¢¯åº¦è®¡ç®—æˆåŠŸ")
        else:
            print("âŒ æ¶æ„å‚æ•° (Alpha) æ— æ¢¯åº¦ï¼")
            
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()